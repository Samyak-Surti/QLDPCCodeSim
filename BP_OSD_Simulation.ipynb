{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run qsu.ipynb\n",
    "import numpy as np\n",
    "import sympy\n",
    "import networkx as nx\n",
    "from qecsim import paulitools as pt \n",
    "from qecsim.models.generic import DepolarizingErrorModel\n",
    "from qecsim.models.toric import ToricCode\n",
    "import nbimporter\n",
    "from typing import List, Tuple\n",
    "import hypergraph_prod_code as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belief_prop(H: np.array, s: np.array, p: float, max_iter: int) -> Tuple:\n",
    "    \"\"\" \n",
    "    Belief Propagation Algorithm for Decoding LDPC Codes\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity-check matrix corresponding to either X or Z checks\n",
    "    s - Error syndrome\n",
    "    p - Channel error rate for chosen noise channel\n",
    "    max_iter - Maximum number of iterations to run BP algorithm for\n",
    "    \"\"\"\n",
    "    data_to_parity = np.zeros((len(H[0]),len(H)), dtype=float)\n",
    "    parity_to_data = np.zeros((len(H), len(H[0])), dtype=float)\n",
    "    H_tanner_graph = hpc.parity_check_mat_to_tanner(H)\n",
    "    \n",
    "    # Channel Log Likelihood Ratio\n",
    "    p_l = np.log((1 - p)/p)\n",
    "    \n",
    "    P_1 = np.zeros((len(H[0]),), dtype=float)\n",
    "    e_BP = np.zeros((len(H[0]),), dtype=float)\n",
    "\n",
    "    # (1) Initialization\n",
    "    for edge in H_tanner_graph.edges:\n",
    "        data_node_num = int(edge[0][1:])\n",
    "        parity_node_num = int(edge[1][1:])\n",
    "        data_to_parity[data_node_num][parity_node_num] = p_l \n",
    "\n",
    "    for iter in range(1, max_iter + 1):\n",
    "        # Scaling Factor\n",
    "        a = 1 - 2**(-1 * iter)\n",
    "\n",
    "        # (2) Parity to Data Messages\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "            data_node_num = int(edge[0][1:])\n",
    "\n",
    "            # Get list of neighbors of current parity_node set minus the current data node\n",
    "            V = list(nx.neighbors(H_tanner_graph, edge[1]))\n",
    "            V.remove(edge[0])\n",
    "\n",
    "            # Get messages from elements of V to current parity node\n",
    "            data_to_par_msgs = [data_to_parity[int(v[1:])][parity_node_num] for v in V]\n",
    "            w = np.min([np.abs(msg) for msg in data_to_par_msgs])\n",
    "            parity_to_data[parity_node_num][data_node_num] = ((-1) ** int(s[parity_node_num])) * a * np.prod(np.sign(data_to_par_msgs)) * w \n",
    "\n",
    "        # (3) Data to Parity Messages\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            data_node_num = int(edge[0][1:])\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "\n",
    "            # Get list of neighbors of current data node set minus the current parity node\n",
    "            U = list(nx.neighbors(H_tanner_graph, edge[0]))\n",
    "            U.remove(edge[1])\n",
    "\n",
    "            # Get messages from elements of U to current data node\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            data_to_parity[data_node_num][parity_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "\n",
    "        \"\"\"\n",
    "        # Hard Decision \n",
    "        for data_node in [node for node in H_tanner_graph.nodes if node[0] == 'v']:\n",
    "            data_node_num = int(data_node[1:])\n",
    "\n",
    "            # Get list of neighbors of current data node \n",
    "            U = list(nx.neighbors(H_tanner_graph, data_node))\n",
    "\n",
    "            # Get messages from elements of U to current data node\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            P_1[data_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "            e_BP[data_node_num] = -1 * np.sign(P_1[data_node_num])\n",
    "        #print(e_BP)\n",
    "        \"\"\"\n",
    "\n",
    "        # Hard Decision\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            data_node_num = int(edge[0][1:])\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "\n",
    "            # Get list of neighbors of current data node\n",
    "            U = list(nx.neighbors(H_tanner_graph, edge[0]))\n",
    "\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            P_1[data_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "            e_BP[data_node_num] = -1 * np.sign(P_1[data_node_num])\n",
    "        \n",
    "        \n",
    "        # (4) Termination Check\n",
    "        e_BP = e_BP * (e_BP > 0)\n",
    "        print(e_BP)\n",
    "        if (np.array_equal(np.dot(H, e_BP), s)):\n",
    "            return True, e_BP, P_1 \n",
    "\n",
    "    return False, e_BP, P_1\n",
    "\n",
    "# Turn to Ordered Statistics Decoding if BP fails to converge\n",
    "def OSD_0(H: np.array, P_1: np.array, s: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    The Ordered Statistics Decoding (OSD) Zero algorithm is a post-processing \n",
    "    algorithm utilized when BP fails to converge \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity check matrix\n",
    "    P_1 - BP soft decision vector\n",
    "    s - Error syndrome \n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Error string\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the rank of the parity check matrix\n",
    "    H_rank = np.linalg.matrix_rank(H)\n",
    "\n",
    "    # Maintain a mapping between bit positions and elements of the BP soft-decision vector\n",
    "    P_1_sorted_pos = np.argsort(P_1, kind='stable')\n",
    "    P_1_sorted = [P_1[i] for i in P_1_sorted_pos]\n",
    "\n",
    "    # Rearrange columns of H to match the reordered soft-decision vector\n",
    "    idx = np.empty_like(P_1_sorted_pos)\n",
    "    H[:] = H[:, idx]\n",
    "\n",
    "    # Select first RANK(H) linearly independent columns of above rearrangement\n",
    "    _, inds = sympy.Matrix(H).rref()\n",
    "    H_S = np.vstack((H[:][inds[i]] for i in range(0, H_rank))).T\n",
    "\n",
    "    # Calculate the OSD-0 solution on the basis-bits\n",
    "    e_S = np.linalg.inv(H_S) * s\n",
    "    e_ST = np.hstack((e_S, np.zeros((len(H[0]) - H_rank,))))\n",
    "\n",
    "    # Map the OSD-0 solution to the original bit-ordering\n",
    "    e_OSD = np.zeros((len(H[0]),))\n",
    "    for i in range(len(P_1_sorted_pos)):\n",
    "        e_OSD[P_1_sorted_pos[i]] = e_ST[i]\n",
    "\n",
    "    return e_OSD\n",
    "\n",
    "\"\"\"\n",
    "# Turn to Ordered Statistics Decoding if BP fails to converge\n",
    "def OSD_0(H: np.array, P_1: np.array, s: np.array) -> np.array:\n",
    "    #S = np.array()\n",
    "\n",
    "    # Get the rank of the parity check matrix\n",
    "    H_rank = rank(matrix(H))\n",
    "    print(H_rank)\n",
    "\n",
    "    # Maintain a mapping between elements of the BP soft-decision vector and bit positions \n",
    "    pos = [i for i in range(len(P_1))]\n",
    "    P_1_dict = {i:p for (i,p) in zip(P_1, pos)}\n",
    "    P_1_sorted = np.sort(P_1)\n",
    "    P_1_sorted_pos = [P_1_dict[p] for p in P_1_sorted]\n",
    "    \n",
    "    # Rearrange columns of H to match the reordered soft-decision vector\n",
    "    idx = np.empty_like(P_1_sorted_pos)\n",
    "    H[:] = H[:, idx]\n",
    "\n",
    "    # Select first RANK(H) linearly independent columns of above rearrangement\n",
    "    _, inds = sympy.Matrix(H).rref()\n",
    "    H_S = np.vstack((H[:][inds[i]] for i in range(0, H_rank))).T\n",
    "\n",
    "    # Calculate the OSD-0 solution on the basis-bits\n",
    "    e_S = np.linalg.inv(H_S) * s\n",
    "    e_ST = np.hstack((e_S, np.zeros((len(H[0]) - H_rank,))))\n",
    "\n",
    "    # Map the OSD-0 solution to the original bit-ordering\n",
    "    e_OSD = np.zeros((len(H[0]),))\n",
    "    for i in range(len(P_1_sorted_pos)):\n",
    "        e_OSD[P_1_sorted_pos[i]] = e_ST[i]\n",
    "\n",
    "    return e_OSD\n",
    "\"\"\"\n",
    "\n",
    "def OSD_0_Plus(H: np.array, P_1: np.array, s: np.array, e_T: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    The Higher Order OSD algorithm is a post-processing \n",
    "    algorithm utilized when BP fails to converge, building \n",
    "    on OSD-0 Algorithm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity check matrix\n",
    "    P_1 - BP soft decision vector\n",
    "    s - Error syndrome \n",
    "    e_T - Choice of error bits on remaining bits that aren't getting flipped with high enough probability\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Error string \n",
    "    \"\"\"\n",
    "    GF = galois.GF(2)\n",
    "    S = np.array()\n",
    "\n",
    "    # Get the rank of the parity check matrix\n",
    "    H_rank = rank(matrix(H))\n",
    "\n",
    "    # Maintain a mapping between elements of the BP soft-decision vector and bit positions \n",
    "    P_1_dict = {p:i for p in P_1 for i in range(len(P_1))}\n",
    "    P_1_sorted = np.sort(P_1)\n",
    "    P_1_sorted_pos = [P_1_dict[p] for p in P_1_sorted]\n",
    "    \n",
    "    # Rearrange columns of H to match the reordered soft-decision vector\n",
    "    idx = np.empty_like(P_1_sorted_pos)\n",
    "    H[:] = H[:, idx]\n",
    "\n",
    "    # Select first RANK(H) linearly independent columns of above rearrangement\n",
    "    _, inds = sympy.Matrix(H).rref()\n",
    "    H_S = np.vstack((H[:][inds[i]] for i in range(0, H_rank))).T\n",
    "    H_T = np.vstack(H[:][inds[i]] for i in range (H_rank, len(H[0])))\n",
    "\n",
    "    # Calculate the OSD-0 solution on the basis-bits\n",
    "    e_S = np.linalg.inv(H_S) * s\n",
    "    e_ST_1 = GF(np.linalg.inv(H_S) * e_S + np.linalg.inv(H_S) * H_T * e_T)\n",
    "    e_ST_2 = GF(e_T)\n",
    "    e_ST = np.hstack((e_ST_1, e_ST_2))\n",
    "\n",
    "    # Map the OSD-0 solution to the original bit-ordering\n",
    "    e_OSD = np.zeros((len(H[0]),))\n",
    "    for i in range(len(P_1_sorted_pos)):\n",
    "        e_OSD[P_1_sorted_pos[i]] = e_ST[i]\n",
    "\n",
    "    return e_OSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models\n",
    "dim = 3\n",
    "my_code = ToricCode(dim,dim)\n",
    "my_error_model = DepolarizingErrorModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "error:\n",
      "┼─·─┼─·─┼─·\n",
      "Y   ·   ·  \n",
      "┼─·─┼─X─┼─·\n",
      "·   X   ·  \n",
      "┼─·─┼─·─┼─·\n",
      "·   ·   ·  \n"
     ]
    }
   ],
   "source": [
    "# Set physical error probability to 10%\n",
    "error_probability = 0.1\n",
    "# Seed random number generator for repeatability\n",
    "rng = np.random.default_rng(27)\n",
    "\n",
    "# Error: random error based on error probability\n",
    "error = my_error_model.generate(my_code, error_probability, rng)\n",
    "print(error)\n",
    "print(('error:\\n{}'.format(my_code.new_pauli(error))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      "syndrome:\n",
      "X───┼───┼──\n",
      "│ Z │ Z │ Z\n",
      "X───┼───┼──\n",
      "│ Z │   │  \n",
      "┼───┼───┼──\n",
      "│   │   │  \n"
     ]
    }
   ],
   "source": [
    "# Syndrome: Stabilizers that do not commute with the error\n",
    "syndrome = pt.bsp(error, my_code.stabilizers.T)\n",
    "print(syndrome)\n",
    "print(('syndrome:\\n{}'.format(my_code.ascii_art(syndrome))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_error: [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "X_error: [0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Z_stabs = (my_code.stabilizers[:dim ** 2])[:, 2 * dim ** 2:]\n",
    "X_stabs = (my_code.stabilizers[dim ** 2:])[:, :2 * dim ** 2]\n",
    "Z_error = error[2 * dim**2:]\n",
    "X_error = error[:2 * dim**2]\n",
    "print(\"Z_error: \" + str(Z_error))\n",
    "print(\"X_error: \" + str(X_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_syndrome: [1 0 0 0 0 0 1 0 0]\n",
      "X_syndrome: [1 1 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Z_syndrome = np.mod(X_stabs @ Z_error,2)\n",
    "print(\"Z_syndrome: \" + str(Z_syndrome))\n",
    "X_syndrome = np.mod(Z_stabs @ X_error, 2)\n",
    "print(\"X_syndrome: \" + str(X_syndrome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0.  1. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[-0. -0. -0.  1. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[-0. -0. -0.  1. -0. -0. -0. -0. -0. -0. -0.  1. -0. -0. -0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "converged_Z, e_BP_Z, P_1_Z = belief_prop(X_stabs, Z_syndrome, 0.1, 2 * dim ** 2)\n",
    "converged_X, e_BP_X, P_1_X = belief_prop(Z_stabs, X_syndrome, 0.1, 2 * dim ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [-0. -0. -0. -0. -0. -0. -0. -0. -0.  1. -0. -0. -0. -0. -0. -0. -0. -0.] [ 3.84514301  3.84514301  3.84514301  3.84514301  3.84514301  3.84514301\n",
      "  5.49306144  7.14097988  5.49306144 -2.74653072  3.84514301  3.84514301\n",
      "  2.19722458  5.49306144  5.49306144  2.19722458  5.49306144  5.49306144]\n",
      "\n",
      "\n",
      "True [-0. -0. -0.  1. -0. -0. -0. -0. -0. -0. -0.  1. -0. -0. -0. -0. -0. -0.] [ 3.63915321  3.63915321  3.63915321 -1.64791843  2.19722458  2.19722458\n",
      "  2.19722458  4.60043896  4.60043896  1.23593882  1.23593882 -0.2059898\n",
      "  2.19722458  2.19722458  4.60043896  4.60043896  4.60043896  4.60043896]\n"
     ]
    }
   ],
   "source": [
    "print(converged_Z, e_BP_Z, P_1_Z)\n",
    "print('\\n')\n",
    "print(converged_X, e_BP_X, P_1_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4612130128693887755 is out of bounds for axis 1 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/012l939x26zc05mjshslzh2h0000gn/T/ipykernel_8520/2996642970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me_OSD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOSD_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_stabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_syndrome\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5d/012l939x26zc05mjshslzh2h0000gn/T/ipykernel_8520/2768575017.py\u001b[0m in \u001b[0;36mOSD_0\u001b[0;34m(H, P_1, s)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# Rearrange columns of H to match the reordered soft-decision vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_1_sorted_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# Select first RANK(H) linearly independent columns of above rearrangement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4612130128693887755 is out of bounds for axis 1 with size 18"
     ]
    }
   ],
   "source": [
    "e_OSD = OSD_0(X_stabs, P_1, Z_syndrome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('topo_quant')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4046caf55f50b54fa7ffdea1ad98452a8feb9cf62ccba5e4c76fe622e38f8fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
