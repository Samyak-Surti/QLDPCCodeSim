{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run qsu.ipynb\n",
    "import numpy as np\n",
    "import sympy\n",
    "import networkx as nx\n",
    "from qecsim import paulitools as pt \n",
    "from qecsim.models.generic import DepolarizingErrorModel\n",
    "from qecsim.models.toric import ToricCode\n",
    "import nbimporter\n",
    "import galois\n",
    "from typing import List, Tuple\n",
    "import hypergraph_prod_code as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belief_prop(H: np.array, s: np.array, p: float, max_iter: int) -> Tuple:\n",
    "    \"\"\" \n",
    "    Belief Propagation Algorithm for Decoding LDPC Codes\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity-check matrix corresponding to either X or Z checks\n",
    "    s - Error syndrome\n",
    "    p - Channel error rate for chosen noise channel\n",
    "    max_iter - Maximum number of iterations to run BP algorithm for\n",
    "    \"\"\"\n",
    "    data_to_parity = np.zeros((len(H[0]),len(H)), dtype=float)\n",
    "    parity_to_data = np.zeros((len(H), len(H[0])), dtype=float)\n",
    "    H_tanner_graph = hpc.parity_check_mat_to_tanner(H)\n",
    "    \n",
    "    # Channel Log Likelihood Ratio\n",
    "    p_l = np.log((1 - p)/p)\n",
    "    \n",
    "    P_1 = np.zeros((len(H[0]),), dtype=float)\n",
    "    e_BP = np.zeros((len(H[0]),), dtype=float)\n",
    "\n",
    "    # (1) Initialization\n",
    "    for edge in H_tanner_graph.edges:\n",
    "        data_node_num = int(edge[0][1:])\n",
    "        parity_node_num = int(edge[1][1:])\n",
    "        data_to_parity[data_node_num][parity_node_num] = p_l \n",
    "\n",
    "    for iter in range(1, max_iter + 1):\n",
    "        # Scaling Factor\n",
    "        a = 1 - 2**(-1 * iter)\n",
    "\n",
    "        # (2) Parity to Data Messages\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "            data_node_num = int(edge[0][1:])\n",
    "\n",
    "            # Get list of neighbors of current parity_node set minus the current data node\n",
    "            V = list(nx.neighbors(H_tanner_graph, edge[1]))\n",
    "            V.remove(edge[0])\n",
    "\n",
    "            # Get messages from elements of V to current parity node\n",
    "            data_to_par_msgs = [data_to_parity[int(v[1:])][parity_node_num] for v in V]\n",
    "            w = np.min([np.abs(msg) for msg in data_to_par_msgs])\n",
    "            parity_to_data[parity_node_num][data_node_num] = ((-1) ** int(s[parity_node_num])) * a * np.prod(np.sign(data_to_par_msgs)) * w \n",
    "\n",
    "        # (3) Data to Parity Messages\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            data_node_num = int(edge[0][1:])\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "\n",
    "            # Get list of neighbors of current data node set minus the current parity node\n",
    "            U = list(nx.neighbors(H_tanner_graph, edge[0]))\n",
    "            U.remove(edge[1])\n",
    "\n",
    "            # Get messages from elements of U to current data node\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            data_to_parity[data_node_num][parity_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "\n",
    "        # Hard Decision\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            data_node_num = int(edge[0][1:])\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "\n",
    "            # Get list of neighbors of current data node\n",
    "            U = list(nx.neighbors(H_tanner_graph, edge[0]))\n",
    "\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            P_1[data_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "            e_BP[data_node_num] = -1 * np.sign(P_1[data_node_num])\n",
    "        \n",
    "        # (4) Termination Check\n",
    "        e_BP = e_BP * (e_BP > 0)\n",
    "        print(e_BP)\n",
    "        if (np.array_equal(np.dot(H, e_BP), s)):\n",
    "            return True, e_BP, P_1 \n",
    "\n",
    "    return False, e_BP, P_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod(x,modulus):\n",
    "    numer, denom = x.as_numer_denom()\n",
    "    return numer*sympy.mod_inverse(denom,modulus) % modulus\n",
    "\n",
    "# Turn to Ordered Statistics Decoding if BP fails to converge\n",
    "def OSD_0(H: np.array, P_1: np.array, s: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    The Ordered Statistics Decoding (OSD) Zero algorithm is a post-processing \n",
    "    algorithm utilized when BP fails to converge \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity check matrix\n",
    "    P_1 - BP soft decision vector\n",
    "    s - Error syndrome \n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Error string\n",
    "    \"\"\"\n",
    "    GF = galois.GF(2)\n",
    "    # Get the rank of the parity check matrix\n",
    "    H_rank = np.linalg.matrix_rank(GF(H))\n",
    "    print(H_rank)\n",
    "\n",
    "    # Maintain a mapping between bit positions and elements of the BP soft-decision vector\n",
    "    P_1_sorted_pos = np.argsort(P_1, kind='stable')[::-1]\n",
    "    P_1_sorted = [P_1[i] for i in P_1_sorted_pos]\n",
    "    print(P_1_sorted_pos)\n",
    "    print(P_1_sorted)\n",
    "\n",
    "    # Rearrange columns of H to match the reordered soft-decision vector\n",
    "    H[:] = H[:, P_1_sorted_pos]\n",
    "\n",
    "    # Select first RANK(H) linearly independent columns of above rearrangement\n",
    "    H_rref, inds = sympy.Matrix(H).rref()\n",
    "    #print(inds)\n",
    "\n",
    "    H_rref = H_rref.applyfunc(lambda x : mod(x,2))\n",
    "    H_rref, inds = sympy.Matrix(H_rref).rref()\n",
    "    H_S = np.vstack(([H[:, inds[i]] for i in range(0, H_rank)]))\n",
    "    \n",
    "    H_rref, inds = sympy.Matrix(H).rref()\n",
    "    H_rref = H_rref.applyfunc(lambda x : mod(x,2))\n",
    "    H_rref, inds = sympy.Matrix(H_rref).rref()\n",
    "    H_S = np.vstack(([H_S[:, inds[i]] for i in range(0, H_rank)])).T \n",
    "    H_S_inv = np.mod(np.linalg.inv(H_S), 2)\n",
    "    \n",
    "\n",
    "    # Calculate the OSD-0 solution on the basis-bits\n",
    "    e_S = np.linalg.inv(H_S) @ s\n",
    "    e_ST = np.hstack((e_S, np.zeros((len(H[0]) - H_rank,))))\n",
    "\n",
    "    # Map the OSD-0 solution to the original bit-ordering\n",
    "    e_OSD = np.zeros((len(H[0]),))\n",
    "    for i in range(len(P_1_sorted_pos)):\n",
    "        e_OSD[P_1_sorted_pos[i]] = e_ST[i]\n",
    "\n",
    "    return e_OSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H_J(H:np.array, J: List, s_p: np.array):\n",
    "    if (J == []):\n",
    "        H_J = []\n",
    "    else:\n",
    "        H_J = np.vstack((H[:, i] for i in J)).T\n",
    "    H_J_s = np.hstack((H_J,s_p))\n",
    "    return H_J, H_J_s\n",
    "\n",
    "def replace_pos(e_p: np.array, x: np.array, J: List):\n",
    "    count = 0\n",
    "    for i in range(len(e_p)):\n",
    "        if(i in J):\n",
    "            e_p[i] = x[count]\n",
    "            count += 1\n",
    "\n",
    "    return e_p\n",
    "\n",
    "\n",
    "# Turn to Ordered Statistics Decoding if BP fails to converge\n",
    "def OSD_0_V2(H: np.array, s: np.array, e_p: np.array, num_qubits: int) -> np.array:\n",
    "    \"\"\" \n",
    "    The Ordered Statistics Decoding (OSD) Zero algorithm is a post-processing \n",
    "    algorithm utilized when BP fails to converge \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - Parity check matrix\n",
    "    s - Error syndrome \n",
    "    e_p - Hard decision vector\n",
    "    num_qubits - Number of qubits\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Error string e such that He = s\n",
    "    \"\"\"\n",
    "    # Want to construct information set J\n",
    "    J = []\n",
    "    s_p = s + H @ e_p\n",
    "    for i in range(num_qubits):\n",
    "        H_J, H_J_s = get_H_J(H, J, s_p)\n",
    "        if (np.linalg.matrix_rank(H_J_s) == np.linalgn.matrix_rank(H_J)):\n",
    "            break \n",
    "        J_p = J + [i]\n",
    "        H_J_p, _ = get_H_J(H, J, s_p)\n",
    "        if (np.linalg.matrix_rank(H_J_p) > np.linalg.matrix_rank(H_J)):\n",
    "            J = J_p\n",
    "            s_p += e_p[i] * H[:,i]\n",
    "    rref, _ = scipy.Matrix.rref(scipy.Matrix(H_J_s))\n",
    "    x = rref[:,-1]\n",
    "    e = replace_pos(e_p, x, s_p)\n",
    "    return e\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59724212 0.14237857 0.16801868 0.62005519]\n",
      " [0.26144282 0.23197529 0.97146369 0.22740526]\n",
      " [0.64196107 0.52270575 0.32546935 0.27299961]]\n",
      "[[0.83349352]\n",
      " [0.41004976]\n",
      " [0.90393023]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.59724212, 0.14237857, 0.16801868, 0.62005519, 0.83349352],\n",
       "       [0.26144282, 0.23197529, 0.97146369, 0.22740526, 0.41004976],\n",
       "       [0.64196107, 0.52270575, 0.32546935, 0.27299961, 0.90393023]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = np.random.random((3,4))\n",
    "print(H)\n",
    "s = np.random.random((3,1))\n",
    "print(s)\n",
    "np.hstack((H,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models\n",
    "dim = 3\n",
    "my_code = ToricCode(dim,dim)\n",
    "my_error_model = DepolarizingErrorModel()\n",
    "GF = galois.GF(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "error:\n",
      "┼─·─┼─·─┼─·\n",
      "·   Z   ·  \n",
      "┼─·─┼─·─┼─·\n",
      "·   ·   ·  \n",
      "┼─·─┼─·─┼─·\n",
      "·   ·   ·  \n"
     ]
    }
   ],
   "source": [
    "# Set physical error probability to 10%\n",
    "error_probability = 0.1\n",
    "# Seed random number generator for repeatability\n",
    "#rng = np.random.default_rng(15355)\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Error: random error based on error probability\n",
    "error = my_error_model.generate(my_code, error_probability, rng)\n",
    "print(error)\n",
    "print(('error:\\n{}'.format(my_code.new_pauli(error))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "syndrome:\n",
      "┼───X───┼──\n",
      "│   │   │  \n",
      "┼───X───┼──\n",
      "│   │   │  \n",
      "┼───┼───┼──\n",
      "│   │   │  \n"
     ]
    }
   ],
   "source": [
    "# Syndrome: Stabilizers that do not commute with the error\n",
    "syndrome = pt.bsp(error, my_code.stabilizers.T)\n",
    "print(syndrome)\n",
    "print(('syndrome:\\n{}'.format(my_code.ascii_art(syndrome))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1]]\n",
      "[[0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1]\n",
      " [1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]]\n",
      "[[0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]]\n",
      "Z_error: [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "X_error: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Z_stabs = (my_code.stabilizers[:dim ** 2])[:, 2 * dim ** 2:]\n",
    "X_stabs = (my_code.stabilizers[dim ** 2:])[:, :2 * dim ** 2]\n",
    "print(str(Z_stabs) + \"\\n\" + str(X_stabs))\n",
    "print(np.mod(X_stabs @ Z_stabs.T,2))\n",
    "Z_error = error[2 * dim**2:]\n",
    "X_error = error[:2 * dim**2]\n",
    "print(\"Z_error: \" + str(Z_error))\n",
    "print(\"X_error: \" + str(X_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_syndrome: [0 1 0 0 0 0 0 1 0]\n",
      "X_syndrome: [0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Z_syndrome = np.mod(X_stabs @ Z_error,2)\n",
    "print(\"Z_syndrome: \" + str(Z_syndrome))\n",
    "X_syndrome = np.mod(Z_stabs @ X_error, 2)\n",
    "print(\"X_syndrome: \" + str(X_syndrome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0.  1. -0. -0. -0. -0. -0. -0. -0.]\n",
      "[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "converged_Z, e_BP_Z, P_1_Z = belief_prop(X_stabs, Z_syndrome, 0.1, 2 * dim ** 2)\n",
    "converged_X, e_BP_X, P_1_X = belief_prop(Z_stabs, X_syndrome, 0.1, 2 * dim ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [-0. -0. -0. -0. -0. -0. -0. -0. -0. -0.  1. -0. -0. -0. -0. -0. -0. -0.] [ 3.84514301  3.84514301  3.84514301  3.84514301  3.84514301  3.84514301\n",
      "  5.49306144  5.49306144  7.14097988  3.84514301 -2.74653072  3.84514301\n",
      "  5.49306144  2.19722458  5.49306144  5.49306144  2.19722458  5.49306144]\n",
      "\n",
      "\n",
      "True [-0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0. -0.] [4.39444915 4.39444915 4.39444915 4.39444915 4.39444915 4.39444915\n",
      " 4.39444915 4.39444915 4.39444915 4.39444915 4.39444915 4.39444915\n",
      " 4.39444915 4.39444915 4.39444915 4.39444915 4.39444915 4.39444915]\n"
     ]
    }
   ],
   "source": [
    "print(converged_Z, e_BP_Z, P_1_Z)\n",
    "print('\\n')\n",
    "print(converged_X, e_BP_X, P_1_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('topo_quant')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4046caf55f50b54fa7ffdea1ad98452a8feb9cf62ccba5e4c76fe622e38f8fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
