{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belief Propagation + Ordered Statistics Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import galois\n",
    "import sympy\n",
    "import qiskit\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanner_to_parity_check_mat(G: nx.Graph) -> np.array:\n",
    "    \"\"\"\n",
    "    Converts Tanner graph 'G' into parity check matrix\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    G - Tanner graph\n",
    "    \"\"\"\n",
    "    num_var_nodes = 0\n",
    "    num_check_nodes = 0\n",
    "    check_node_list = []\n",
    "    nodes_dict = {}\n",
    "    G_nodes = list(G.nodes)\n",
    "    G_edges = list(G.edges)\n",
    "    for node in G_nodes:\n",
    "        if (node[0] == 'v'):\n",
    "            nodes_dict[node] = num_var_nodes\n",
    "            num_var_nodes += 1\n",
    "        else:\n",
    "            nodes_dict[node] = num_check_nodes\n",
    "            num_check_nodes += 1\n",
    "    \n",
    "    mat = np.zeros((num_check_nodes, num_var_nodes))\n",
    "    for edge in G_edges:\n",
    "        if (edge[0][0] == 'c'):\n",
    "            mat[nodes_dict[edge[0]]][nodes_dict[edge[1]]] = 1\n",
    "        else:\n",
    "            mat[nodes_dict[edge[1]]][nodes_dict[edge[0]]] = 1\n",
    "            \n",
    "    return mat\n",
    "\n",
    "def parity_check_mat_to_tanner(mat: np.array) -> nx.Graph:\n",
    "    \"\"\" \n",
    "    Converts parity check matrix 'mat' into Tanner graph\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    mat - parity check matrix\n",
    "    \"\"\"\n",
    "    num_var_nodes = mat.shape[1]\n",
    "    num_check_nodes = mat.shape[0]\n",
    "    tan_graph = nx.Graph();\n",
    "    for i in range(num_var_nodes):\n",
    "        tan_graph.add_node(\"v\" + str(i))\n",
    "\n",
    "    for j in range(num_check_nodes):\n",
    "        tan_graph.add_node(\"c\" + str(j))\n",
    "\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat.shape[1]):\n",
    "            if (mat[i][j] == 1):\n",
    "                tan_graph.add_edge(\"c\" + str(i), \"v\" + str(j))\n",
    "    \n",
    "    return tan_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belief_prop(H: np.array, s: np.array, p: float, max_iter: int) -> Tuple:\n",
    "    \"\"\" \n",
    "    Belief Propagation Algorithm for Decoding LDPC Codes\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity-check matrix corresponding to either X or Z checks\n",
    "    s - Error syndrome\n",
    "    p - Channel error rate for chosen noise channel\n",
    "    max_iter - Maximum number of iterations to run BP algorithm for\n",
    "    \"\"\"\n",
    "    data_to_parity = np.zeros((len(H[0]),len(H)), dtype=float)\n",
    "    parity_to_data = np.zeros((len(H), len(H[0])), dtype=float)\n",
    "    H_tanner_graph = parity_check_mat_to_tanner(H)\n",
    "    \n",
    "    # Channel Log Likelihood Ratio\n",
    "    p_l = np.log((1 - p)/p)\n",
    "    P_1 = np.zeros((len(H[0]),), dtype=float)\n",
    "    e_BP = np.zeros((len(H[0]),), dtype=float)\n",
    "\n",
    "    # (1) Initialization\n",
    "    for edge in H_tanner_graph.edges:\n",
    "        data_node_num = int(edge[0][1:])\n",
    "        parity_node_num = int(edge[1][1:])\n",
    "        data_to_parity[data_node_num][parity_node_num] = p_l \n",
    "\n",
    "    for iter in range(1, max_iter + 1):\n",
    "        # Scaling Factor\n",
    "        a = 1 - 2**(-1 * iter)\n",
    "\n",
    "        # (2) Parity to Data Messages\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "            data_node_num = int(edge[0][1:])\n",
    "\n",
    "            # Get list of neighbors of current parity_node set minus the current data node\n",
    "            V = list(nx.neighbors(H_tanner_graph, edge[1]))\n",
    "            V.remove(edge[0])\n",
    "\n",
    "            # Get messages from elements of V to current parity node\n",
    "            data_to_par_msgs = [data_to_parity[int(v[1:])][parity_node_num] for v in V]\n",
    "            w = np.min([np.abs(msg) for msg in data_to_par_msgs])\n",
    "            parity_to_data[parity_node_num][data_node_num] = ((-1) ** s[parity_node_num]) * a * np.prod(np.sign(data_to_par_msgs)) * w \n",
    "\n",
    "        # (3) Data to Parity Messages\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            data_node_num = int(edge[0][1:])\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "\n",
    "            # Get list of neighbors of current data node set minus the current parity node\n",
    "            U = list(nx.neighbors(H_tanner_graph, edge[0]))\n",
    "            U.remove(edge[1])\n",
    "\n",
    "            # Get messages from elements of U to current data node\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            data_to_parity[data_node_num][parity_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "\n",
    "        # Hard Decision \n",
    "        for data_node in [node for node in H_tanner_graph.nodes if node[0] == 'v']:\n",
    "            data_node_num = int(data_node[1:])\n",
    "\n",
    "            # Get list of neighbors of current data node \n",
    "            U = list(nx.neighbors(H_tanner_graph, data_node))\n",
    "\n",
    "            # Get messages from elements of U to current data node\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            P_1[data_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "            e_BP[data_node_num] = -1 * np.sign(P_1[data_node_num])\n",
    "        \n",
    "        \n",
    "        # (4) Termination Check\n",
    "        if (np.array_equal(np.dot(H, e_BP), s)):\n",
    "            return True, e_BP, P_1 \n",
    "\n",
    "    return False, e_BP, P_1\n",
    "\n",
    "# Turn to Ordered Statistics Decoding if BP fails to converge\n",
    "def OSD_0(H: np.array, P_1: np.array, s: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    The Ordered Statistics Decoding (OSD) Zero algorithm is a post-processing \n",
    "    algorithm utilized when BP fails to converge \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity check matrix\n",
    "    P_1 - BP soft decision vector\n",
    "    s - Error syndrome \n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Error string\n",
    "    \"\"\"\n",
    "    S = np.array()\n",
    "\n",
    "    # Get the rank of the parity check matrix\n",
    "    H_rank = rank(matrix(H))\n",
    "\n",
    "    # Maintain a mapping between elements of the BP soft-decision vector and bit positions \n",
    "    P_1_dict = {p:i for p in P_1 for i in range(len(P_1))}\n",
    "    P_1_sorted = np.sort(P_1)\n",
    "    P_1_sorted_pos = [P_1_dict[p] for p in P_1_sorted]\n",
    "    \n",
    "    # Rearrange columns of H to match the reordered soft-decision vector\n",
    "    idx = np.empty_like(P_1_sorted_pos)\n",
    "    H[:] = H[:, idx]\n",
    "\n",
    "    # Select first RANK(H) linearly independent columns of above rearrangement\n",
    "    _, inds = sympy.Matrix(H).rref()\n",
    "    H_S = np.vstack((H[:][inds[i]] for i in range(0, H_rank))).T\n",
    "\n",
    "    # Calculate the OSD-0 solution on the basis-bits\n",
    "    e_S = np.linalg.inv(H_S) * s\n",
    "    e_ST = np.hstack((e_S, np.zeros((len(H[0]) - H_rank,))))\n",
    "\n",
    "    # Map the OSD-0 solution to the original bit-ordering\n",
    "    e_OSD = np.zeros((len(H[0]),))\n",
    "    for i in range(len(P_1_sorted_pos)):\n",
    "        e_OSD[P_1_sorted_pos[i]] = e_ST[i]\n",
    "\n",
    "    return e_OSD\n",
    "\n",
    "def OSD_0_Plus(H: np.array, P_1: np.array, s: np.array, e_T: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    The Higher Order OSD algorithm is a post-processing \n",
    "    algorithm utilized when BP fails to converge, building \n",
    "    on OSD-0 Algorithm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity check matrix\n",
    "    P_1 - BP soft decision vector\n",
    "    s - Error syndrome \n",
    "    e_T - Choice of error bits on remaining bits that aren't getting flipped with high enough probability\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Error string \n",
    "    \"\"\"\n",
    "    GF = galois.GF(2)\n",
    "    S = np.array()\n",
    "\n",
    "    # Get the rank of the parity check matrix\n",
    "    H_rank = rank(matrix(H))\n",
    "\n",
    "    # Maintain a mapping between elements of the BP soft-decision vector and bit positions \n",
    "    P_1_dict = {p:i for p in P_1 for i in range(len(P_1))}\n",
    "    P_1_sorted = np.sort(P_1)\n",
    "    P_1_sorted_pos = [P_1_dict[p] for p in P_1_sorted]\n",
    "    \n",
    "    # Rearrange columns of H to match the reordered soft-decision vector\n",
    "    idx = np.empty_like(P_1_sorted_pos)\n",
    "    H[:] = H[:, idx]\n",
    "\n",
    "    # Select first RANK(H) linearly independent columns of above rearrangement\n",
    "    _, inds = sympy.Matrix(H).rref()\n",
    "    H_S = np.vstack((H[:][inds[i]] for i in range(0, H_rank))).T\n",
    "    H_T = np.vstack(H[:][inds[i]] for i in range (H_rank, len(H[0])))\n",
    "\n",
    "    # Calculate the OSD-0 solution on the basis-bits\n",
    "    e_S = np.linalg.inv(H_S) * s\n",
    "    e_ST_1 = GF(np.linalg.inv(H_S) * e_S + np.linalg.inv(H_S) * H_T * e_T)\n",
    "    e_ST_2 = GF(e_T)\n",
    "    e_ST = np.hstack((e_ST_1, e_ST_2))\n",
    "\n",
    "    # Map the OSD-0 solution to the original bit-ordering\n",
    "    e_OSD = np.zeros((len(H[0]),))\n",
    "    for i in range(len(P_1_sorted_pos)):\n",
    "        e_OSD[P_1_sorted_pos[i]] = e_ST[i]\n",
    "\n",
    "    return e_OSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_pauli_error(num_qubits: int, r:float) -> Tuple:\n",
    "    \"\"\"\n",
    "    Generate a random pauli error on 'num_qubits' qubits\n",
    "    \"\"\"\n",
    "    error_op_X = [None for i in range(num_qubits)]\n",
    "    error_op_Z = [None for i in range(num_qubits)] \n",
    "    for i in range(num_qubits):\n",
    "        z = np.random.uniform()\n",
    "        if (z >= 0 and z <= r/3):\n",
    "            error_op_X[i], error_op_Z[i] = 1, 0\n",
    "        elif (z > r/3 and z <= (2*r)/3):\n",
    "            error_op_X[i], error_op_Z[i] = 1, 1\n",
    "        elif (z > (2 * r)/3 and z <= r):\n",
    "            error_op_X[i], error_op_Z[i] = 0, 1\n",
    "        else:\n",
    "            error_op_X[i], error_op_Z[i] = 0, 0\n",
    "    return error_op_X, error_op_Z\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('qc_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a2db16169323f9c61c6ed4d885397ad0494f2e790734ab97aaeab1bbd202ff7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
