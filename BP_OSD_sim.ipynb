{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belief Propagation + Ordered Statistics Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import galois\n",
    "import sympy\n",
    "import nbimporter\n",
    "import stim\n",
    "import hypergraph_prod_code as hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belief_prop(H: np.array, s: np.array, p: float, max_iter: int) -> Tuple:\n",
    "    \"\"\" \n",
    "    Belief Propagation Algorithm for Decoding LDPC Codes\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity-check matrix corresponding to either X or Z checks\n",
    "    s - Error syndrome\n",
    "    p - Channel error rate for chosen noise channel\n",
    "    max_iter - Maximum number of iterations to run BP algorithm for\n",
    "    \"\"\"\n",
    "    data_to_parity = np.zeros((len(H[0]),len(H)), dtype=float)\n",
    "    parity_to_data = np.zeros((len(H), len(H[0])), dtype=float)\n",
    "    H_tanner_graph = hpc.parity_check_mat_to_tanner(H)\n",
    "    \n",
    "    # Channel Log Likelihood Ratio\n",
    "    p_l = np.log((1 - p)/p)\n",
    "    \n",
    "    P_1 = np.zeros((len(H[0]),), dtype=float)\n",
    "    e_BP = np.zeros((len(H[0]),), dtype=float)\n",
    "\n",
    "    # (1) Initialization\n",
    "    for edge in H_tanner_graph.edges:\n",
    "        data_node_num = int(edge[0][1:])\n",
    "        parity_node_num = int(edge[1][1:])\n",
    "        data_to_parity[data_node_num][parity_node_num] = p_l \n",
    "\n",
    "    for iter in range(1, max_iter + 1):\n",
    "        # Scaling Factor\n",
    "        a = 1 - 2**(-1 * iter)\n",
    "\n",
    "        # (2) Parity to Data Messages\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "            data_node_num = int(edge[0][1:])\n",
    "\n",
    "            # Get list of neighbors of current parity_node set minus the current data node\n",
    "            V = list(nx.neighbors(H_tanner_graph, edge[1]))\n",
    "            V.remove(edge[0])\n",
    "\n",
    "            # Get messages from elements of V to current parity node\n",
    "            data_to_par_msgs = [data_to_parity[int(v[1:])][parity_node_num] for v in V]\n",
    "            w = np.min([np.abs(msg) for msg in data_to_par_msgs])\n",
    "            parity_to_data[parity_node_num][data_node_num] = ((-1) ** int(s[parity_node_num])) * a * np.prod(np.sign(data_to_par_msgs)) * w \n",
    "\n",
    "        # (3) Data to Parity Messages\n",
    "        for edge in H_tanner_graph.edges:\n",
    "            data_node_num = int(edge[0][1:])\n",
    "            parity_node_num = int(edge[1][1:])\n",
    "\n",
    "            # Get list of neighbors of current data node set minus the current parity node\n",
    "            U = list(nx.neighbors(H_tanner_graph, edge[0]))\n",
    "            U.remove(edge[1])\n",
    "\n",
    "            # Get messages from elements of U to current data node\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            data_to_parity[data_node_num][parity_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "\n",
    "        # Hard Decision \n",
    "        for data_node in [node for node in H_tanner_graph.nodes if node[0] == 'v']:\n",
    "            data_node_num = int(data_node[1:])\n",
    "\n",
    "            # Get list of neighbors of current data node \n",
    "            U = list(nx.neighbors(H_tanner_graph, data_node))\n",
    "\n",
    "            # Get messages from elements of U to current data node\n",
    "            par_to_data_msgs = [parity_to_data[int(u[1:])][data_node_num] for u in U]\n",
    "            P_1[data_node_num] = p_l + np.sum(par_to_data_msgs)\n",
    "            e_BP[data_node_num] = -1 * np.sign(P_1[data_node_num])\n",
    "        print(e_BP)\n",
    "        \n",
    "        \n",
    "        # (4) Termination Check\n",
    "        if (np.array_equal(np.dot(H, e_BP), s)):\n",
    "            return True, e_BP, P_1 \n",
    "\n",
    "    return False, e_BP, P_1\n",
    "\n",
    "\n",
    "# Turn to Ordered Statistics Decoding if BP fails to converge\n",
    "def OSD_0(H: np.array, P_1: np.array, s: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    The Ordered Statistics Decoding (OSD) Zero algorithm is a post-processing \n",
    "    algorithm utilized when BP fails to converge \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity check matrix\n",
    "    P_1 - BP soft decision vector\n",
    "    s - Error syndrome \n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Error string\n",
    "    \"\"\"\n",
    "    #S = np.array()\n",
    "\n",
    "    # Get the rank of the parity check matrix\n",
    "    H_rank = rank(matrix(H))\n",
    "    print(H_rank)\n",
    "\n",
    "    # Maintain a mapping between elements of the BP soft-decision vector and bit positions \n",
    "    pos = [i for i in range(len(P_1))]\n",
    "    P_1_dict = {i:p for (i,p) in zip(P_1, pos)}\n",
    "    P_1_sorted = np.sort(P_1)\n",
    "    P_1_sorted_pos = [P_1_dict[p] for p in P_1_sorted]\n",
    "    \n",
    "    # Rearrange columns of H to match the reordered soft-decision vector\n",
    "    idx = np.empty_like(P_1_sorted_pos)\n",
    "    H[:] = H[:, idx]\n",
    "\n",
    "    # Select first RANK(H) linearly independent columns of above rearrangement\n",
    "    _, inds = sympy.Matrix(H).rref()\n",
    "    H_S = np.vstack((H[:][inds[i]] for i in range(0, H_rank))).T\n",
    "\n",
    "    # Calculate the OSD-0 solution on the basis-bits\n",
    "    e_S = np.linalg.inv(H_S) * s\n",
    "    e_ST = np.hstack((e_S, np.zeros((len(H[0]) - H_rank,))))\n",
    "\n",
    "    # Map the OSD-0 solution to the original bit-ordering\n",
    "    e_OSD = np.zeros((len(H[0]),))\n",
    "    for i in range(len(P_1_sorted_pos)):\n",
    "        e_OSD[P_1_sorted_pos[i]] = e_ST[i]\n",
    "\n",
    "    return e_OSD\n",
    "\n",
    "def OSD_0_Plus(H: np.array, P_1: np.array, s: np.array, e_T: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    The Higher Order OSD algorithm is a post-processing \n",
    "    algorithm utilized when BP fails to converge, building \n",
    "    on OSD-0 Algorithm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    H - parity check matrix\n",
    "    P_1 - BP soft decision vector\n",
    "    s - Error syndrome \n",
    "    e_T - Choice of error bits on remaining bits that aren't getting flipped with high enough probability\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Error string \n",
    "    \"\"\"\n",
    "    GF = galois.GF(2)\n",
    "    S = np.array()\n",
    "\n",
    "    # Get the rank of the parity check matrix\n",
    "    H_rank = rank(matrix(H))\n",
    "\n",
    "    # Maintain a mapping between elements of the BP soft-decision vector and bit positions \n",
    "    P_1_dict = {p:i for p in P_1 for i in range(len(P_1))}\n",
    "    P_1_sorted = np.sort(P_1)\n",
    "    P_1_sorted_pos = [P_1_dict[p] for p in P_1_sorted]\n",
    "    \n",
    "    # Rearrange columns of H to match the reordered soft-decision vector\n",
    "    idx = np.empty_like(P_1_sorted_pos)\n",
    "    H[:] = H[:, idx]\n",
    "\n",
    "    # Select first RANK(H) linearly independent columns of above rearrangement\n",
    "    _, inds = sympy.Matrix(H).rref()\n",
    "    H_S = np.vstack((H[:][inds[i]] for i in range(0, H_rank))).T\n",
    "    H_T = np.vstack(H[:][inds[i]] for i in range (H_rank, len(H[0])))\n",
    "\n",
    "    # Calculate the OSD-0 solution on the basis-bits\n",
    "    e_S = np.linalg.inv(H_S) * s\n",
    "    e_ST_1 = GF(np.linalg.inv(H_S) * e_S + np.linalg.inv(H_S) * H_T * e_T)\n",
    "    e_ST_2 = GF(e_T)\n",
    "    e_ST = np.hstack((e_ST_1, e_ST_2))\n",
    "\n",
    "    # Map the OSD-0 solution to the original bit-ordering\n",
    "    e_OSD = np.zeros((len(H[0]),))\n",
    "    for i in range(len(P_1_sorted_pos)):\n",
    "        e_OSD[P_1_sorted_pos[i]] = e_ST[i]\n",
    "\n",
    "    return e_OSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depolarizing_error(num_qubits: int, r:float) -> Tuple:\n",
    "    \"\"\"\n",
    "    Generate a random pauli error on 'num_qubits' qubits\n",
    "    \"\"\"\n",
    "    error_op_X = np.array([None for i in range(num_qubits)])\n",
    "    error_op_Z = np.array([None for i in range(num_qubits)])\n",
    "    for i in range(num_qubits):\n",
    "        z = np.random.uniform()\n",
    "        if (z >= 0 and z <= r/3):\n",
    "            error_op_X[i], error_op_Z[i] = 1, 0\n",
    "        elif (z > r/3 and z <= (2*r)/3):\n",
    "            error_op_X[i], error_op_Z[i] = 1, 1\n",
    "        elif (z > (2 * r)/3 and z <= r):\n",
    "            error_op_X[i], error_op_Z[i] = 0, 1\n",
    "        else:\n",
    "            error_op_X[i], error_op_Z[i] = 0, 0\n",
    "    return error_op_X, error_op_Z\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Cell to check for commutation of stabilizers of Toric Code\n",
    "def toPauliStringX(pc_mat_X: np.array) -> List:\n",
    "    result = []\n",
    "    for elem in pc_mat_X:\n",
    "        p = \"\"\n",
    "        for i in range(len(elem)):\n",
    "            if (elem[i] == 0):\n",
    "                p += \"I\" \n",
    "            else:\n",
    "                p += \"X\"\n",
    "        result.append(stim.PauliString(p))\n",
    "    return result\n",
    "    \n",
    "def toPauliStringZ(pc_mat_Z: np.array) -> List:\n",
    "    result = []\n",
    "    for elem in pc_mat_Z:\n",
    "        p = \"\"\n",
    "        for i in range(len(elem)):\n",
    "            if (elem[i] == 0):\n",
    "                p += \"I\" \n",
    "            else:\n",
    "                p += \"Z\"\n",
    "        result.append(stim.PauliString(p))\n",
    "    return result\n",
    "\n",
    "def checkCommutation(all_stabs: List):\n",
    "    commutes = True\n",
    "    for i in range(len(all_stabs)):\n",
    "        for j in range(len(all_stabs)):\n",
    "            if (i == j):\n",
    "                continue \n",
    "            commutes = commutes and all_stabs[i].commutes(all_stabs[j])\n",
    "    return commutes\n",
    "\n",
    "mat = np.array([\n",
    "    [1,1,0],\n",
    "    [0,1,1],\n",
    "    [1,0,1]\n",
    "], dtype=int)\n",
    "toric_code_X, toric_code_Z = hpc.construct_hypergraph_prod_code(mat, mat)\n",
    "\n",
    "x_stabs = toPauliStringX(toric_code_X)\n",
    "z_stabs = toPauliStringZ(toric_code_Z)\n",
    "all_stabs = x_stabs + z_stabs \n",
    "print(checkCommutation(all_stabs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-type error string is: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "X-type syndrome is: [0 1 1 0 0 0 0 0 0]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -0. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "False [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.] [ 40.58149257  39.58281987  39.58281987  40.58149257  39.58281987\n",
      "  39.58281987  42.14736324  39.0156219   39.0156219   38.01694921\n",
      " -37.75291408  38.01694921  40.58149257  39.0156219   40.58149257\n",
      "  40.58149257  39.0156219   40.58149257]\n",
      "9\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4630908154647582669 is out of bounds for axis 1 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/012l939x26zc05mjshslzh2h0000gn/T/ipykernel_16688/4227566449.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_sol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarg_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconverged\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0me_OSD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOSD_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoric_code_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarg_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyndrome_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5d/012l939x26zc05mjshslzh2h0000gn/T/ipykernel_16688/1051135944.py\u001b[0m in \u001b[0;36mOSD_0\u001b[0;34m(H, P_1, s)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Rearrange columns of H to match the reordered soft-decision vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_1_sorted_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Select first RANK(H) linearly independent columns of above rearrangement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4630908154647582669 is out of bounds for axis 1 with size 18"
     ]
    }
   ],
   "source": [
    "#GF = galois.GF(2)\n",
    "\n",
    "mat = np.array([\n",
    "    [1,1,0],\n",
    "    [0,1,1],\n",
    "    [1,0,1]\n",
    "], dtype=int)\n",
    "toric_code_X, toric_code_Z = hpc.construct_hypergraph_prod_code(mat, mat)\n",
    "error_str_X, _ = depolarizing_error(18, 0.08)\n",
    "print(\"X-type error string is: \" + str(error_str_X))\n",
    "#syndrome_X = GF(toric_code_X) @ GF(error_str_X)\n",
    "syndrome_X = np.mod(toric_code_Z @ error_str_X, 2)\n",
    "print(\"X-type syndrome is: \" + str(syndrome_X))\n",
    "converged, bp_sol, marg_prob = belief_prop(toric_code_X, syndrome_X, 0.1, 27)\n",
    "print(converged, bp_sol, marg_prob)\n",
    "if (converged == False):\n",
    "    e_OSD = OSD_0(toric_code_X, marg_prob, syndrome_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.6",
   "language": "sage",
   "name": "SageMath-9.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4046caf55f50b54fa7ffdea1ad98452a8feb9cf62ccba5e4c76fe622e38f8fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
